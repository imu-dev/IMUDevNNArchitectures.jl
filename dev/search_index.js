var documenterSearchIndex = {"docs":
[{"location":"pages/tcn/#Temporal-Convolutional-Network-(TCN)","page":"TCN","title":"Temporal Convolutional Network (TCN)","text":"","category":"section"},{"location":"pages/tcn/","page":"TCN","title":"TCN","text":"We implement a Temporal Convolutional Network","category":"page"},{"location":"pages/tcn/","page":"TCN","title":"TCN","text":"note: Note\nTCN is an example of a sequence-to-sequence, non-recurrent architecture. See foreword for more details.","category":"page"},{"location":"pages/tcn/","page":"TCN","title":"TCN","text":"tcn","category":"page"},{"location":"pages/tcn/#IMUDevNNArchitectures.TCN.tcn","page":"TCN","title":"IMUDevNNArchitectures.TCN.tcn","text":"tcn(in_out::Pair{Int,Int};\n    channels::AbstractVector{Int}=[], kernel_size=3, dropout=0.2)\n\nCreate a Temporal Convolutional Network (TCN) model. The model is a chain of tcnblock layers. The number of layers is determined by the length of the channels vector. The first layer has input size first(in_out) and output size channels[1]. The last layer has input size channels[end] and output size last(in_out).\n\nwarning: Warning\nThe original implementation https://github.com/locuslab/TCN uses a technique called Weight Normalization; however, as of today Flux.jl does not implement this (see also this issue).\n\n\n\n\n\n","category":"function"},{"location":"pages/utils/#Utility-functions","page":"utilities","title":"Utility functions","text":"","category":"section"},{"location":"pages/utils/","page":"utilities","title":"utilities","text":"PReLU","category":"page"},{"location":"pages/utils/#IMUDevNNArchitectures.PReLU","page":"utilities","title":"IMUDevNNArchitectures.PReLU","text":"PReLU(α::Real=0.25f0)\n\nParametric Rectified Linear Unit (PReLU). Activation function of the form:\n\nbegincases\n    x  textif  x  0 \n    αx  textotherwise\nendcases\n\nwhere α is a learnable parameter.\n\nExample\n\nusing Flux\nusing IMUDevNNArchitectures\n\nmodel = Chain(\n    Dense(10, 5),\n    PReLU(0.25f0),\n    Dense(5, 2),\n    softmax)\n\n\n\n\n\n","category":"type"},{"location":"pages/general_note/#Note-on-NN-architectures-for-temporal-data","page":"Foreword","title":"Note on NN architectures for temporal data","text":"","category":"section"},{"location":"pages/general_note/","page":"Foreword","title":"Foreword","text":"The Neural Net architectures designed for Temporal Data have two ways of categorization:","category":"page"},{"location":"pages/general_note/","page":"Foreword","title":"Foreword","text":"In terms of what can they predict:\nsequence-to-point predictors: these take a sequence of timepoints and output a point-predictor (which could contain more than one label).\nsequence-to-sequence predictors: these output a predictor label for each timepoint (possibly excluding some warm-up segment).\nIn terms of how the data are passed through them:\nrecurrent architecture: data points are sequentially fed through a network and a label is output for every time point.\nnon-recurrent architecture: data points are collected into segments and passed jointly through a network.","category":"page"},{"location":"#IMUDevNNArchitectures.jl","page":"Home","title":"IMUDevNNArchitectures.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements some common Neural Network Architectures for temporal-type data.","category":"page"},{"location":"pages/resnet/#Residual-Network-(ResNet-1d)","page":"ResNet","title":"Residual Network (ResNet 1d)","text":"","category":"section"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"We implement 1d Residual Networks.","category":"page"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"note: Note\nResNet is an example of a sequence-to-point, non-recurrent architecture. See foreword for more details.","category":"page"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"resnet","category":"page"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.resnet","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.resnet","text":"resnet(in_out::Pair{Tuple{Int,Int},Int}, group_sizes;\n       strides=vcat([1], fill(2, length(group_sizes) - 1)),\n       base_plane::Int=64, kernel_size::Int=3,\n       outputblock_builder=(in_out) -> outputblock(in_out;\n                                                   hidden=1024,\n                                                   dropout=0.5,\n                                                   transition_size=0))\n\nResNet1d model (see the original ResNet publication). It consists of an input block, a residual block, and an output block. The residual block is defined by the group_sizes, strides, base_plane and kernel_size. The output block is often custom built, but a default one is provided. in_out argument in the outputblock_builder is a pair of the form (num_channels, time_dim) => output_dim.\n\nArguments\n\nin_out::Pair{Tuple{Int,Int},Int}: The input and output sizes of the model.\ngroup_sizes: The number of basic blocks in each residual group.\nstrides: The stride for the first basic block in each residual group.\nbase_plane::Int: Hyperparameter based on which the number of channels in each residual group is going to be computed.\nkernel_size::Int: The size of the kernel in each basic block.\noutputblock_builder: A function that builds the output block of the model.\n\nnote: Note\nThe in_out has a format (num_channels, time_dim) => output_dim.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"IMUDevNNArchitectures.ResNet1d.outputblock","category":"page"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.outputblock","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.outputblock","text":"outputblock(in_out::Pair{Tuple{Int,Int},Int};\n            hidden::Int=1024,\n            dropout::Float64=0.5,\n            transition_size::Int=0)\n\nOutput block for ResNet1d. It can have a transition layer between the residual block and the output layer.\n\nArguments\n\nin_out::Pair{Tuple{Int,Int},Int}: The input and output sizes of the block.\nhidden::Int: The size of the fully connected hidden layers in the output layer.\ndropout::Float64: The dropout rate for the output layer.\ntransition_size::Int: The size of the transition layer.\n\nnote: Note\nThe in_out has a format (num_channels, time_dim) => output_dim.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"IMUDevNNArchitectures.ResNet1d.transition_layer\nIMUDevNNArchitectures.ResNet1d.output_layer","category":"page"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.transition_layer","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.transition_layer","text":"transition_layer(in_out::Pair{Int,Int})\n\nTransition layer for ResNet1d that can be stuck between the residual block and an output layer. It consists of a convolutional layer and a batch normalization layer.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.output_layer","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.output_layer","text":"output_layer(in_out::Pair{Int,Int};\n             hidden::Int=1024,\n             dropout::Float64=0.5)\n\nOutput layer for ResNet1d. hidden is the size of the fully connected hidden layers.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"IMUDevNNArchitectures.ResNet1d.init!\nIMUDevNNArchitectures.ResNet1d.zero_init!","category":"page"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.init!","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.init!","text":"init!(m::Chain)\n\nInitialize the weights and biases of the resnet model m.\n\nnote: Note\nIf you'd like to zero initialize the weights of the last BatchNormalization in each residual branch of the model, use the function zero_init! after calling this function.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.zero_init!","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.zero_init!","text":"Zero-initialize the last BatchNormalization in each residual branch of the model m. This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677.\n\nwarning: Warning\nCurrent implementation is based on the assumption that all SkipConnections in the model m have the same form as the one in basicblock and thus have the BatchNormalization layer as the fourth layer. If this is not the case, the function will not work as expected.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"IMUDevNNArchitectures.ResNet1d.inputblock\nIMUDevNNArchitectures.ResNet1d.residualblock","category":"page"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.inputblock","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.inputblock","text":"inputblock(in_out::Pair{Int,Int})\n\nInput block for ResNet1d. It consists of a convolutional layer, a batch normalization layer, and a max pooling layer.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.residualblock","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.residualblock","text":"residualblock(baseplane::Int, groupsizes, strides; kernel_size::Int=3)\n\nDefines the residual block for ResNet1d. It consists of residual groups stacked together.\n\nArguments\n\nbase_plane::Int: Hyperparameter based on which the number of channels in each residual group is going to be computed.\ngroup_sizes: The number of basic blocks in each residual group.\nstrides: The stride for the first basic block in each residual group.\nkernel_size::Int: The size of the kernel in each basic block.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/","page":"ResNet","title":"ResNet","text":"IMUDevNNArchitectures.ResNet1d.residualgroup\nIMUDevNNArchitectures.ResNet1d.basicblock","category":"page"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.residualgroup","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.residualgroup","text":"residualgroup(kernel_size::Int, in_out::Pair{Int,Int};\n              group_size::Int, stride::Int=1)\n\nResidual group for ResNet1d. It consists of basic blocks stacked together. Only the first basic block in the group can have a stride greater than 1.\n\nArguments\n\nkernel_size::Int: The size of the kernel in each basic block.\nin_out::Pair{Int,Int}: The input and output channels of the group.\ngroup_size::Int: The number of basic blocks in the group.\nstride::Int: The stride for the first basic block in the group.\n\n\n\n\n\n","category":"function"},{"location":"pages/resnet/#IMUDevNNArchitectures.ResNet1d.basicblock","page":"ResNet","title":"IMUDevNNArchitectures.ResNet1d.basicblock","text":"basicblock(kernel_size::Int, in_out::Pair{Int,Int}; stride::Int)\n\nA basic block for ResNet1d. It consists of two convolutional layers with batch normalization and a skip connection. ResNet1d will stack these basic blocks to form residual groups (see also residualgroup).\n\nArguments\n\nkernel_size::Int: The size of the kernel for the convolutional layers.\nin_out::Pair{Int,Int}: The input and output channels of the block.\nstride::Int: The stride for the first convolutional layer.\n\n\n\n\n\n","category":"function"}]
}
